<?xml version="1.0" encoding="utf-8"?>
<ul>
  <li>zip函数zip()函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用*号操作符，可以将元组解压为列表。用法：123zip([iterable,...])#返回一个元组对象*zip()#用于解压缩，可以在机器学习中用于打乱数据123456789101112X=[1,2,3,4,5,6]y=[0,1,0,0,1,1]zipped_data=list(zip(X,y))#将样本和标签一一对应组合起来,并转换成list类型方便后续打乱操作random.shuffle(zipped_data)#使用random模块中的shuffle函数打乱列表，原地操作，没有返回值shuffle本身也是洗牌的意思new_zipped_data=list(map(list,zip(*zipped_data)))#zip(*)反向解压，map()逐项转换类型，list()做最后转换new_X,new_y=new_zipped_data[0],new_zipped_data[1]#返回打乱后的新数据print('X:',X,'\n','y:',y)print('new_X:',new_X,'\n','new_y:',new_y)</li>
  <li>因为准备复试,学习一下微机原理,第一篇笔记记录一下思维导图</li>
  <li>1不保留原有文件此法可能会造成被合并盘符内文件丢失，不过大多数情况下应该可以通过数据恢复找回文件另外需要说明的是，这里的所谓的合并磁盘本质上合并在磁盘上个一个一个分区，而不是真正的独立的磁盘打开此电脑（win+E)打开管理打开磁盘管理可以看到我这里显示是磁盘0和磁盘1，代表我的计算机上装有两个实体硬盘，不是同一个实体硬盘不能合并分区（固态硬盘和机械硬盘）删除某磁盘分区这里正好有一个我不需要的分区，可以合并，在你的电脑上应该显示为你想被合并进去的那块磁盘（这块盘数据会丢失）删除完之后会显示为未分配状态合并磁盘在想要合并进去的磁盘分区上点击拓展卷接下来的操作流程：这里可能会被自动添加过去，点击下一步，完成即可。2.保留原有文件https://www.diskgenius.cn/help/partresizing.php参考这篇文章，可能需要vip</li>
  <li>整体等于其部分之和——欧几里得整体大于其部分之和——MaxWertheimer基本概念在计算机视觉领域，图像分割（segmentation）指的是将数字图像细分为多个图像子区域（像素的集合）（也被称作超像素）的过程。图像分割的目的是简化或改变图像的表示形式，使得图像更容易理解和分析。[1]图像分割通常用于定位图像中的物体和边界（线，曲线等）。更精确的，图像分割是对图像中的每个像素加标签的一个过程，这一过程使得具有相同标签的像素具有某种共同视觉特性。图像分割的结果是图像上子区域的集合（这些子区域的全体覆盖了整个图像），或是从图像中提取的轮廓线的集合（例如边缘检测）。一个子区域中的每个像素在某种特性的度量下或是由计算得出的特性都是相似的，例如颜色、亮度、纹理。邻接区域在某种特性的度量下有很大的不同。多数图像分割算法基于灰度值的两个基本性质之一：不连续性以灰度突变为基础分割一幅图像，比如图像的边缘。相似性根据一组预定义的准则讲一幅图像分割为相似的区域。如：阈值处理、区域生长、区域分裂、区域聚合</li>
  <li>白苎新袍入嫩凉。春蚕食叶响回廊，禹门已准桃花浪，月殿先收桂子香。鹏北海，凤朝阳。又携书剑路茫茫。明年此日青云去，却笑人间举子忙。为什么考研首要问题是提高就业竞争力，我并没有瞧不起我的学校，我只是认为我所在的学校给我提供的平台并没有那么让我满意，包括软硬件条件，教学水平等等。考上研究生之后，至少可以提升平台，更容易的找到一份心仪的工作。当然我并不抗拒去工作，事实上我也学了不少东西，去找个小厂当当码农还是可以的。其次是为了解开我的一个心结，高考翻车的痛隐隐约约疼了快4年，不管怎样，考一下，了了这个心结。经验之谈关于择校不谈，主要讲关于公共课的复习。1.数学早期（3月到6月）一直在看汤家凤，后来感觉汤家凤的课过于简单，至少基础阶段是这样，但是总体来讲汤家凤的课还是非常适合打基础的，主要问题在于1800（一本习题册），这本习题册着实不敢恭维，答案跳步很严重，不适合基础阶段做题。所以基础阶段做660比较好，如果我二战了，二话不说先买一本660做。暑假阶段（7月到8月）一直在看李正元的复习全书，这本书真的很难，我啃这本书花了很多时间，但是效果也不是特别好，所以不是特别推荐暑假阶段啃这个，最好多做一些题，这个阶段在打好基础的前提下做330是很不错的。后期（9月到12月）就是真题与模拟卷，真题我买了李正元的书，感觉并不是特别好，因为收录不全，只收录了2004~2019年的题目，所以最好是买张宇的真题大全解，这本书对真题的收录非常全面。其次，关于模拟卷，我是做完一遍真题（10月中下旬）才开始做模拟卷的，第一套就做了张宇八套卷，真特么劝退，但是建议早做这套卷子，后期劝退就相当难受了。模拟卷大概做了这些：张宇八套卷，四套卷李林6套卷，四套卷李正元5套卷（题很难，做的不好）合工大超越5套卷，共创5套卷合工大2015~2020的模拟卷，挑着做了一部分。我虽然做了这么多卷子，但是效果并不好，还是建议做好做精几套卷子，不要贪多。至于老师，推荐张宇、李林、吴忠祥，其他的没听过课。2.英语英语主要是背单词，我从二月中旬开始背单词，一直背到考研前一天都没有停过，一直在用墨墨背单词，受益颇多，早期每天会背200个单词（新学+复习），后期就慢慢得少了。再者就是做真题，真题我开始得很早，大概四月就开始做阅读，只做了阅读，别的都没做。大概到了9月才开始学三小门（完型，新题型，翻译），难度不大，阅读频率慢慢降下来。我大概到了10月左右才开始看唐迟的阅读视频，感觉很不错，作文也是10月开始看的刘晓燕的视频，记了很多笔记，感觉也并没有多大用处。。。自我感觉基础很好，跳过了长难句和语法阶段。3.政治8月开始复习即可，完全不需要早开始，政治早期复习完全就是在看选择题，如果学数学学累了可以看看腿姐的视频，看马原就够了，其他的不用看。肖秀荣的书，1000题，肖四，肖八是必须必须要买的，其他的随意。做1000题建议用Anki（一个记忆辅助类软件），这样，1000题其实也不用买了。分析题到了11月下旬在开始也不晚，因为政治分析题全国只有一个考生，就是肖秀荣，今年的分析题肖老押题押得非常给力，几乎全中，我只是大概考前几天背了背肖四（研木易的精缩版本），分析题也没有花别的时间了。4专业课各校不一样，不过根据我今年考试的情况，建议全面复习，不留任何知识盲区（即使是非常冷门的考点），这点我想对任何学校都是适用的。5.其他资料的获取这里推荐多加一些考研的群和公众号，到最后就会发现各种资料是满天飞的可以去B站搜或者去公众号搜，直接搜考研就可以，多关注一些，慢慢的公众号就全都是考研相关的东西了。电子设备推荐买iPad+Pencil的组合，的的确确是生产力工具最后，能不能考上看天意了。祈祷上岸。🙏🙏🙏</li>
  <li>他的与众不同之处在于，是透过光线看阴影还是透过阴影看光线​——大卫林赛1.什么是直方图均衡化直方图均衡化本质上属于一种灰度变换与空间滤波技术的范畴，以像素为基本操作单位，对数字图像进行处理。直方图均衡化，顾名思义，就是对直方图进行均衡的一种算法，是一种对图像对比度进行调整的算法2.为什么要进行直方图均衡化有些时候我们得到的图形会因为一系列原因产生与实际需求相悖的图像，比如下面：这幅图像看起来就像是被蒙上了一层雾，图像的可视性受到了一定的损害，为此，需要对图像进行处理，使之动态范围变大，提高对比度，经过直方图变换后的图像如下：当然这只是对图像进行均衡的其中一个原因，具体的原因需要和实际需求相结合，毕竟，工程领域做什么都要与实际需求结合。3.如何进行直方图均衡化以一个8×8的灰度图像为例：该灰度图像的灰度值出现次数如下表所示，为了简化表格，出现次数为0的值已经被省略灰度值出现次数灰度值出现次数灰度值出现次数灰度值出现次数灰度值出现次数521642721852113155365373287112215826627518811261593671761901144160168577194115416146937811042  6217047921061  6327128311091  累积分布函数（cdf）如下所示，与上一表格类似，为了简化，累积分布函数值为0的灰度值已经被省略灰度值cdf灰度值cdf灰度值cdf灰度值cdf灰度值cdf5216419724085511136055465227342875212261586662475438853126625996725764490541446360106830774594551546461146933784610457  62157037794810658  63177139834910959  如表格所示，灰度值最小值为52，最大值为154.通常，直方图均衡化算式如下:累积分布函数最小值cdfmin在本例中为1，最大值cdfmax在本例中为64，而L则是灰度级数（如本例中，图像为8位深度，则灰度级数共有2^8=256级数,这也是最常见的灰度级数）.则对于本例的直方图均衡化算式为:例如，灰度为78的像素的累积分布函数为46，均衡化后，灰度值变化为：直方图均衡化后，图像的灰度值变化如下表所示：图像效果：从灰度直方图上来看，就是集中分布在某一个灰度区间上分布展开的一个过程，对#2中给出的图像进行距离说明，对于before，其直方图如下：其中直方图为红色部分，黑色部分为累积直方图对于变换后的图像灰度分布如下：可以看出，直方图均衡化将累积分布进行了线性化操作，将灰度直方图分布在灰度级上均匀展开，增加了图像的动态范围和对比度，可视性增强。4.实现原理考虑一个离散的灰度图像{x}（二维信号），让ni表示灰度i出现的次数，这样图像中灰度为i的像素的出现概率是L是图像中所有的灰度数，n是图像中所有的像素数，$p_x(i)$实际上是像素值为i的直方图的归一化形式，把对应于$p_x$的累积分布函数，定义为：是图像的累计归一化直方图。我们创建一个形式为y=T(x)的变换，对于原始图像中的每个值它就产生一个y，这样y的累计概率函数就可以在所有值范围内进行线性化，公式如下：再将这些值逆映射到初试的域即可。参考文献：wikipedia直方图均衡化知乎</li>
  <li>很快就能整完</li>
  <li>终于是花了三四天时间断断续续看完了cs224nlecture2，难度真的是指数型上升。基本原理词嵌入什么是词嵌入？自然语言不像图像信息一样，自然语言的基本单位是文字，在英文中是word，中文中是字，他们都是有具体意义的抽象表示，而所谓图像处理，处理的本身就是图像信号，图像本身在采集或者创建好就是以数字形式存储在计算中。但是，重点来了，要使用统计的方法处理自然语言，就必须以一定的形式量化编码词，说简单一点，就是把符号形式的文字转化成数值，或者说嵌入到一个数学空间里，建立一个从文字到数学空间的映射。也就是所谓的词嵌入，而这里要将到的word2vec便是常用的词嵌入的一种。也就是说，词嵌入就是把词进行了数值化的表示。如何进行词嵌入？word2vec是一种典型的词嵌入方法，使用一个全连接网络（输入层，隐含层，输出层）得到一个训练好的可以表征词向量的模型。wrod2vec的一些具体含义比较直观的理解比较通俗的解释方法是，我们把一个词转化成所谓向量，也就是把这个词进行了量化表示，一个比较直观的例子，颜色。我们都知道，颜色在计算机中通常使用rgb，也就是红绿蓝三原色进行编码，很自然的可以通过一个三维向量，比如蓝色（0，0，255），这就是一个经过编码的空间向量，但如果我们不知道所谓的蓝色的向量化表示呢？这就是word2vec的意义所在，将蓝色这个直观意义上（假设）的词语(word)，映射为一个三维向量空间的向量（vector），也就是我们通常所称的rgb色彩空间。也就是：蓝色-&gt;（0，0，255）​红色-&gt;(255，0，0）绿色-&gt;（0，255，0）假设上边三个向量是我们训练出来的，而不是一种基于确定标准的量化，那么有意思的事情来了。我们知道，蓝色+绿色=青色，那么，如果我们拿青色这个词语放到神经网络里面去训练，得出的对应青色的向量，应该是大致就是（0，255，255），这就意味着，我们通过神经网络，找到了所谓的颜色之间的量化关系（大致上）也就是完成了所谓的词嵌入。当然，事实上，颜色并不是这么来的，图像信息作为一种天然可以被计算机量化的信息（曝光），原则上来说根本不需要这么个对颜色词语的词嵌入，这里只是为了举例。类似的，当我们拥有下面的词向量时：“Beijing”、”Tokyo”、”China”、”Japan”。如果训练的足够合理的话，我们应该能大致得到如下的关系\(\vec{Beijing}-\vec{China}+\vec{Japan}=\vec{Tokyo}\)所以，词向量中蕴含是词语之间真实关系的表述，也就是说，当北京有首都的属性和中国的属性，当二者相减时，我们就可以神奇的得到了一个可以表征首都属性的向量，很明显，当加上日本时，就出现了Tokyo。这就是word2vec的强大之处，将客观的词语进行向量化，从而使得原本单一的词语字符串可以进行数学运算，也就是实现了词嵌入。一些需要注意的问题我们通过神经网络训练出来的词向量，理论上来说，我们并不容易知道每个维度其对应的确切意义所在。Manning在第一节课上也讲了这一点，这也就是所谓的黑箱（BlackBox），黑箱这个东西的问题就在于，如果我们对黑箱的输入得到了正确的输出，那很好；但如果我们得到了错误的输出，那对于一般系统来说，可以通过调整参数来对系统进行修正，但调整参数来修正黑箱的性能似乎并不容易。典型的黑箱系统就是神经网络。如果我们仅仅只是要求了大量数据样本下的整体准确率，那很好，一个优秀的神经网络能够很高的准确率与性能，而且设计起来的成本也较一般传统方法大大降低。但是如果除了错就有可能造成不可挽回的后果，那么就必须谨慎使用这个东西了。所以，神经网络的可解释性也是一个非常重要的话题。这个东西，毕竟是一个基于统计学习训练出来的模型。（个人看法）网络原理神经网络基本结构这部分确实看着很复杂，下面用例子简单说明我们假设要训练一个十个词语的word2vec{‘apple’,’drink’,’eat’,’juice’,’milk’,’orange’,’rice’,’water’}我们基本的网络结构如下：如图所示，该网络是一个由10个输入节点的输入层，5个隐含节点的隐含层，10个输出节点的输出层的全连接网络。参数推导(翻译自XinRong的论文arXiv:1411.2738v4[cs.CL]5Jun2016)我们以连续单上下文词预测单目标词的CBOW（ContinuousBag-of-WordModel连续词袋模型）进行参数的推导演示。在输入层和输出层之前的权重矩阵可以用一个\(V\timesX\)的矩阵来\(W\)来表示，\(W\)的每行，都是一个\(N\)维的向量记作\(v_{\omega}\)，对于\(W\)的第\(i\)行，对于给定上下文，假设\(x_k=x_{k'}\)，对于每个\(k'\neqk\)，我们有\(h=w^Tx=W^T_{k,.}:=v^T_{w_I}\tag1\)即本质上就是\(W\)对于\(h\)的第\(k\)行的复制，\(V_{w_{I}}\)是输入词的\(w_I\)的向量表示。这意味着隐含层神经元的激活函数是简单的线性函数。如图：从隐含层到输出层存在另一个大小为\(N\timesV\)权重矩阵\(W'={w'_{ij}}\)，使用这个矩阵，我们可以计算出每个词汇在整个词汇表中的分数值（score）,\(u_j=v_{w_j}^Th\tag2\)其中\(v'_{w_j}\)是矩阵\(W'\)的第j列。然后，我们可以使用soft-max，一个log-linear分类器模型,来获得随后词汇的分布结果，它是一个多项式形式的分布。\(p(w_j|w_I)=y_j=\frac{exp(u_j)}{\sum_{j'=1}^{V}{u_j'}}\tag3\)其中\(y_j\)是输出层第j个神经元的输出，(1)式，(2)式代入(3)中，我们就可以得到如下式子：\(p(w_j|w_I)=\frac{v'^T_{wj}v_{wI}}{\sum_{j'=1}^{V}exp(v'^{T}_{wj'}v_{wI})}\)注意：\(v_w\)和\(v'_w\)是\(w\)的两种表示方式。\(v_w\)来自输入层到隐含层的权重矩阵W的行，而\(v'_w\)来自隐含层到输出层的权重矩阵W的列。在随后的分析中，我们把\(v_w\)称作词w的”输入矩阵”，\(v'_w\)称作词w的输出矩阵。随后，我们就可以使用梯度下降法进行参数更新和训练。直观演示这里使用了XinRong论文中给的wevi进行直观演示（上文中的图也是）网络结构（如上图网络结构）训练数据(contexttarget)1eat|apple,eat|orange,eat|rice,drink|juice,drink|milk,drink|water,orange|juice,apple|juice,rice|milk,milk|drink,water|drink,juice|drink初始状态（如上图所示矩阵）当我们训练500次时，两权重矩阵的变化如下：注意观察，这里orange和juice的词汇向量很相似，也就说明了这两个自然词汇之间存在的关系，基于我们的常识，orangejuice是一种非常合适的组合。类似的，网络结构中可以看出：由上图也可以看出，drink和milk、juice、water同时出现的概率较大，这也符合自然经验。其他诸如隐含层节点的一些有意思的性质，可以自行前往常识。参考资料：XinRongword2vecParameterLearningExplainedhttps://zhuanlan.zhihu.com/p/26306795https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469https://www.youtube.com/watch?v=D-ekE-Wlcds&amp;feature=youtu.beRongXin于2017年驾驶飞机失事，感谢他对人类做出的科研贡献R.I.P.</li>
  <li>本周下午两点，在1#实验楼314举行了一次技术交流讲座，由boboliu同学主讲init初始化阶段似乎出了一些问题，311的投影仪并不能正常工作，最终选择了使用实验室最有钱的人的（boboliu）自带的投影仪。git的使用虽然效果不是特别好，但是，能用。大致讲解了如下几点：git基本命令本地仓库和远端不同步的解决方法如何解决冲突github-pages的使用我们知道，写博客对于程序员来说是一个很好的习惯，github提供的pages静态站点托管服务可以很好的解决这一问题。boboliu同学大致演示了基本流程：fork其他人的jekyllrepo修改配置使用markdown写博客conclusion第一次似乎不是那么成功，不过大家也学到了不少东西。blogsModerRASboboliuhuangzhiweizuoyuzzy阴阳songsiyanliumengxianshiweichendingshuaikang</li>
  <li>Introduction自然是简单介绍，有一些有意思的概念大致写一下。0.语音处理与nlp​语音处理已经做得很不错了，可以看得出来，包括siri，Cortana之类的语音助手，在将语音信号转化为文字这方面还是很不错的，以前感觉它们算是一整个大体系的前端与后端的关系，但似乎并不是，nlp更加关注的如何用计算机处理既有问语言文字数据，包括理解这句话到底什么意思，或者这句话的发出者的情感是怎么样的。这并不是一件容易的事情，就像Manning在slides里面提到了例子：​ThePope’sbabystepsongays.这句话可以理解为教皇的baby走上gays，也可以理解为教皇在对gays的观念上走出了babysteps,显然后者才是正确的，作为人当然可以很容易的区分，但是对于计算机来说似乎就不是一件多么容易的事情了。1.DeepLearning&amp;nlp似乎又拿传统与现代比较，结合了DL的nlp似乎变得又非常强了，就像拥有了cnn的CV一样，还没有具体讲到底是什么，大致讲了一下词向量，就是用向量表示单词，这个就很灵性了，大概就像这样：\(nlp=[0.453,0.441,0.745,0.118,0.4154]^T\)当被问道每个维度具体的意义是，Manning的回答就比较有意思了，一般来说，并不知道每个维度的意义，只能是尝试着去理解每个维度的意义，果然黑箱一路货色。所以，虽然没有具体了解一些东西，但是猜着是一通数据拟合fit？2.Whynlphard？简单来说，语言本身就存在一定的不确定性，自然语言并不像计算机语言一般有着严格确定的规则和语法甚至可以编译成一条一条机器指令去执行，自然语言在具有混沌性？是与上下文以及语境惯量的。我认为，就像slides里漫画中讲的：Languageisn’taformalsystem,languageisgloriouschaos语言并不是一套规则的系统，语言是一种华丽的混沌。</li>
  <li>​于是确实是去了北戴河。而且是跟团去的，质量属实一般吧，稍微记录一下。Day1凌晨几乎没怎么睡得着，大概三点左右被醒来跟旺旺回合，到boc跟hcy和zty回合，去济宁，大巴凌晨5点出发。泰安收费站见到了彩虹，似乎是个好的兆头，事实上并不算是。下午三点左右到了天津津门故里，随便转了转，搞到了三天内用的水壶（张家水铺），看了看泥人张，hcy买了盒花烟，后来抽了一下，顿时成了社会人。然后是山海关，只看了外景，还可以，晚餐属实一般，回到旅馆又加了一餐鱿鱼啤酒，还不错。Day2下着小雨去了某个沙滩，我忘了叫啥了。。。反正就淌了一下海水，沙滩上好多海蜇。。。我们四个人一把伞都没带来，全都放到宾馆了。第二站鱼岛海洋公园，大致就坐船转了转，做了一下海盗船，搞得我一直打嗝，旋转木马也走了一波，特技表演也一般般，最后的薰衣草捏影环节倒还不错。晚上是碧螺岛公园，30块钱看了个失恋博物馆属实憨憨，这次还不如不拦着hcy去走迷宫。Day3去了个珍珠展览馆，标准的购物场所，没意思。Conclusion​感受一般，应该多花点钱去玩个好的。</li>
  <li>填个坑，以后会写的。</li>
  <li>黑镜第五季只有三集，断断续续花了两天时间看完了，今天还考过了科目二，有点开心，稍微写点感受海报莫名的有点可爱。Part1.整体感受本季的黑镜首先相比前两季短了不少，也没有那么NSFW了，也不像之前剧集里浓重的CyperPunk风格。三集下来整个主人公的生活环境就像是跟我们平时生活的一样，当然，其中确实有那么一些科幻的元素，比如episode1里面的VR（ormrar？这个连感官体验都有了，whatever..），这三级都很明显的在探讨一些社会问题，没想到UK也跟我们一样有这么些问题。。。话说现在黑镜被Netflix买下来了？不过质量有所下降。。。那算是美剧了？Part2.分集观感episode1StrikingVipers（生死搏击）猎鹰的幸福生活，黑人老哥还是很有型，影片头十几分钟完全看不出来科幻的感觉，这几集黑镜都是如此，威尔和妻子结婚过着幸福美满的生活，不过曾经一起玩游戏的好基友带来的超强VR（估计叫VR），我也是真的佛了，男人的灵魂进了女人的身体，大致讨论的就是就是夫妻关系？夫妇之间经历了七年之痒需要互相释放一下？最后的两人互相原谅了似乎，但是给了妻子hookup的机会就有点怪怪的了，毕竟vr里面再爽也是假的，为了跟基友一夜欢愉选择了绿一下自己？episode2Smithereens(细碎生活)本集进入节奏还是挺快的，但是刚开始始终没明白到底是为了什么，似乎男主和CEO有仇，这一集还挺有讽刺性和警示性的。男主冲动的原因根本是因为刷了一下社交应用的一条动态，还仅仅是关于一条狗的动态，因此丢掉了心爱的妻子的性命，所以，开车一定不要看手机！！！再者，最后应该是被崩了吧。。。真的，可恨之人必有可怜之处啊。回想我自己，也是整天被淹没在如潮的信息流之中，接收着精心优化的推荐系统给我推荐的像毒品一般的所谓资讯，真的有用的信息寥寥无几，最讽刺的是，这优秀的推荐系统是我一手训练出来的。以后可以稍微多了解一下推荐系统的东西。episode3Rachel，JackandAshleytoo（瑞秋，杰克和小阿什莉）一开始女主侧脸猛一看真的像ScarlettJohnson！，（事实上是蜘蛛侠里面的那个Betty）这一集我感觉是一般的一集，在那个Ashleytoo被解锁之后我的第一反应竟然是怎么驱动问题。。。学傻了快。Part3.最后虽然黑镜质量一直在下降，但是作为每年的必备环节，明年不管别人怎么喷，我应该还是会去看一下的，就像《世界奇妙物语》一样，今年没有春季篇出了一个雨之特别篇，反正质量也一般般吧。可能今年秋天出了秋季特别篇会再去写一下影评。</li>
  <li>Tensorflow官方出了新的文档，经典的MNIST手写数字识别被换成了衣物识别，还挺有意思的，大概整理记录一下训练你的第一个神经网络对于有一定机器学习知识基础的初学者，MNIST手写数字识别无疑是一个非常棒的选择，这里google似乎又把难度降低了一层，改成了所谓的FASHIONMNIST服装图像进行分类。1.导入相关库12345678910from__future__importabsolute_import,division,print_function,unicode_literals#导入TensorFlow和tf.kerasimporttensorflowastffromtensorflowimportkeras#导入辅助库importnumpyasnpimportmatplotlib.pyplotasplttips:12from__future__import...#意思是把下一个版本的特性导入当前版本2.导入FashionMNIST数据集本指南使用FashionMNIST数据集，其中包含了10个类别中共70,000张灰度图像。图像包含了低分辨率（28x28像素）的单个服装物品，如下所示:FashionMNIST旨在替代传统的MNIST数据集—它经常被作为机器学习在计算机视觉方向的”Hello,World”。MNIST数据集包含手写数字（0,1,2等）的图像，其格式与我们在此处使用的服装相同。本指南使用FashionMNIST进行多样化，因为它比普通的MNIST更具挑战性。两个数据集都相对较小，用于验证算法是否按预期工作。它们是测试和调试代码的良好起点。我们将使用60,000张图像来训练网络和10,000张图像来评估网络模型学习图像分类任务的准确程度。您可以直接从TensorFlow使用FashionMNIST，只需导入并加载数据123fashion_mnist=keras.datasets.fashion_mnist(train_images,train_labels),(test_images,test_labels)=fashion_mnist.load_data()##此处可能需要一些小tricks才能正常下载​加载数据集并返回四个NumPy数组:train_images和train_labels数组是训练集—这是模型用来学习的数据。模型通过测试集进行测试,即test_images与test_labels两个数组。图像是28x28NumPy数组，像素值介于0到255之间。labels是一个整数数组，数值介于0到9之间。这对应了图像所代表的服装的类别:标签类别0T-shirt/top1Trouser2Pullover3Dress4Coat5Sandal6Shirt7Sneaker8Bag9Ankleboot每个图像都映射到一个标签。由于类别名称不包含在数据集中,因此把他们存储在这里以便在绘制图像时使用:1class_names=['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankleboot']3.探索数据让我们在训练模型之前探索数据集的格式。以下显示训练集中有60,000个图像，每个图像表示为28x28像素:1train_images.shape1(60000,28,28)同样，训练集中有60,000个标签:1len(train_labels)160000每个标签都是0到9之间的整数:1train_labels1array([9,0,0,...,3,0,5],dtype=uint8)测试集中有10,000个图像。同样，每个图像表示为28×28像素:1test_images.shape1(10000,28,28)测试集包含10,000个图像标签:1len(test_labels)1100004.数据预处理在训练网络之前必须对数据进行预处理。如果您检查训练集中的第一个图像，您将看到像素值落在0到255的范围内:12345plt.figure()plt.imshow(train_images[0])plt.colorbar()plt.grid(False)plt.show()在馈送到神经网络模型之前，我们将这些值缩放到0到1的范围。为此，我们将像素值值除以255。重要的是，对训练集和测试集要以相同的方式进行预处理:1train_images=train_images/255.0test_images=test_images/255.0显示训练集中的前25个图像，并在每个图像下方显示类名。验证数据格式是否正确，我们是否已准备好构建和训练网络。12345678plt.figure(figsize=(10,10))foriinrange(25):plt.subplot(5,5,i+1)plt.xticks([])plt.yticks([])plt.grid(False)plt.imshow(train_images[i],cmap=plt.cm.binary)plt.xlabel(class_names[train_labels[i]])plt.show()5.构建模型构建神经网络需要配置模型的层，然后编译模型。设置网络层一个神经网络最基本的组成部分便是网络层。网络层从提供给他们的数据中提取表示，并期望这些表示对当前的问题更加有意义大多数深度学习是由串连在一起的网络层所组成。大多数网络层，例如tf.keras.layers.Dense，具有在训练期间学习的参数。1model=keras.Sequential([keras.layers.Flatten(input_shape=(28,28)),keras.layers.Dense(128,activation=tf.nn.relu),keras.layers.Dense(10,activation=tf.nn.softmax)])网络中的第一层,tf.keras.layers.Flatten,将图像格式从一个二维数组(包含着28x28个像素)转换成为一个包含着28*28=784个像素的一维数组。可以将这个网络层视为它将图像中未堆叠的像素排列在一起。这个网络层没有需要学习的参数;它仅仅对数据进行格式化。在像素被展平之后，网络由一个包含有两个tf.keras.layers.Dense网络层的序列组成。他们被称作稠密链接层或全连接层。第一个Dense网络层包含有128个节点(或被称为神经元)。第二个(也是最后一个)网络层是一个包含10个节点的softmax层—它将返回包含10个概率分数的数组，总和为1。每个节点包含一个分数，表示当前图像属于10个类别之一的概率。编译模型在模型准备好进行训练之前，它还需要一些配置。这些是在模型的编译(compile)步骤中添加的:损失函数—这可以衡量模型在培训过程中的准确程度。我们希望将此函数最小化以”驱使”模型朝正确的方向拟合。优化器—这就是模型根据它看到的数据及其损失函数进行更新的方式。评价方式—用于监控训练和测试步骤。以下示例使用准确率(accuracy)，即正确分类的图像的分数。1model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])训练模型训练神经网络模型需要以下步骤:将训练数据提供给模型-在本案例中，他们是train_images和train_labels数组。模型学习如何将图像与其标签关联我们使用模型对测试集进行预测,在本案例中为test_images数组。我们验证预测结果是否匹配test_labels数组中保存的标签。通过调用model.fit方法来训练模型—模型对训练数据进行”拟合”。1model.fit(train_images,train_labels,epochs=5)随着模型训练，将显示损失和准确率等指标。该模型在训练数据上达到约0.88(或88％)的准确度。进行预测通过训练模型，我们可以使用它来预测某些图像。1predictions=model.predict(test_images)在此，模型已经预测了测试集中每个图像的标签。我们来看看第一个预测:1predictions[0]123array([6.6858855e-05,2.5964803e-07,5.3627105e-06,4.5019146e-06,2.7420206e-06,4.7881842e-02,2.3233067e-04,5.4705784e-02,8.5581087e-05,8.9701480e-01],dtype=float32)预测是10个数字的数组。这些描述了模型的”信心”，即图像对应于10种不同服装中的每一种。我们可以看到哪个标签具有最高的置信度值：1np.argmax(predictions[0])19因此，模型最有信心的是这个图像是ankleboot，或者class_names[9]。我们可以检查测试标签，看看这是否正确:1test_labels[0]19我们可以用图表来查看全部10个类别1defplot_image(i,predictions_array,true_label,img):predictions_array,true_label,img=predictions_array[i],true_label[i],img[i]plt.grid(False)plt.xticks([])plt.yticks([])plt.imshow(img,cmap=plt.cm.binary)predicted_label=np.argmax(predictions_array)ifpredicted_label==true_label:color='blue'else:color='red'plt.xlabel("{}{:2.0f}%({})".format(class_names[predicted_label],100*np.max(predictions_array),class_names[true_label]),color=color)defplot_value_array(i,predictions_array,true_label):predictions_array,true_label=predictions_array[i],true_label[i]plt.grid(False)plt.xticks([])plt.yticks([])thisplot=plt.bar(range(10),predictions_array,color="#777777")plt.ylim([0,1])predicted_label=np.argmax(predictions_array)thisplot[predicted_label].set_color('red')thisplot[true_label].set_color('blue')让我们看看第0个图像，预测和预测数组。1i=0plt.figure(figsize=(6,3))plt.subplot(1,2,1)plot_image(i,predictions,test_labels,test_images)plt.subplot(1,2,2)plot_value_array(i,predictions,test_labels)plt.show()1i=12plt.figure(figsize=(6,3))plt.subplot(1,2,1)plot_image(i,predictions,test_labels,test_images)plt.subplot(1,2,2)plot_value_array(i,predictions,test_labels)plt.show()让我们绘制几个图像及其预测结果。正确的预测标签是蓝色的，不正确的预测标签是红色的。该数字给出了预测标签的百分比(满分100)。请注意，即使非常自信，也可能出错。1#绘制前X个测试图像，预测标签和真实标签#以蓝色显示正确的预测，红色显示不正确的预测num_rows=5num_cols=3num_images=num_rows*num_colsplt.figure(figsize=(2*2*num_cols,2*num_rows))foriinrange(num_images):plt.subplot(num_rows,2*num_cols,2*i+1)plot_image(i,predictions,test_labels,test_images)plt.subplot(num_rows,2*num_cols,2*i+2)plot_value_array(i,predictions,test_labels)plt.show()最后，使用训练的模型对单个图像进行预测。1#从测试数据集中获取图像img=test_images[0]print(img.shape)1(28,28)tf.keras模型经过优化，可以一次性对批量,或者一个集合的数据进行预测。因此，即使我们使用单个图像，我们也需要将其添加到列表中:1#将图像添加到批次中，即使它是唯一的成员。img=(np.expand_dims(img,0))print(img.shape)1(1,28,28)现在来预测图像:1predictions_single=model.predict(img)print(predictions_single)12[[6.6858927e-052.5964729e-075.3627055e-064.5019060e-062.7420206e-064.7881793e-022.3233047e-045.4705758e-028.5581087e-058.9701480e-01]]1plot_value_array(0,predictions_single,test_labels)plt.xticks(range(10),class_names,rotation=45)plt.show()model.predict返回一个包含列表的列表，每个图像对应一个列表的数据。获取批次中我们(仅有的)图像的预测:1prediction_result=np.argmax(predictions_single[0])print(prediction_result)19而且，和之前一样，模型预测标签为9。</li>
  <li>RoseisredSomethingisuptoone’stalentPassbyvalueispassbycopyit</li>
  <li>算了，本来想好好写一写解题步骤来着，想着实在是乱七八糟，满心胡乱，不解了，写写想法吧。算是正式做完的第一个题，难度并不大，但是解的一塌糊涂，刚开始想的是降维分析一波，画个雷达图，最后计算所成的面积。。。这是我头一天晚上想得，算是建了个模吧，但是第二天仔细分析了一波发现问题并不简单，参考的别人的论文都是用的层次分析，层次分析隐约记得学spss的时候学过，大概是个树状图？那个好像是聚类分析？学完就忘记了。。。大概做题过程就是整体按照我的想法来的，我很高兴，不过我的想法实在不算优秀，最后论文写的也不好，参考文献算是强硬引用？就只有一个Matlab程序，因为数据基本都是Excel做的。。。spss都不用，真的菜啊论文的话，我写了大部分，但是最后发现不大对，因为刚开始给数据降维的时候加的权，都忘了，也没专门记录下来，导致最后强行在附件构建了一波加权矩阵，尴尬.jpgLatex还是不大会用，环境啊环境，环境好就好在他mlgb，真的不会配啊，什么瘠薄玩意儿，之前给别人莫名其妙配了出来，最后自己还是不会配。误打误撞吧。数学功底还是不好，就像那个山的那个，积分都不会积分，插值后一通瞎搞，现在还没搞清楚到底用trap还是integral2.。。。就这样吧，写出来一篇完整的论文还是很有成就感的一件事情，我愈发感觉我思想上是一个完美主义者，但是行动上是一个懒人，对于自己作品，觉得不错的，会反复的把玩观赏。by入眠ps按照我们建的模，我们应该会被淘汰掉。。。</li>
  <li>Matlab自带的遗传算法工具箱已经将常用的遗传运算命令进行了集成，用户使用很方便。但是封装的工具箱内部命令不能根据特殊需要进行相应调整和修改。从这个角度上来说，具有人工智能性质的GAToolbox是一种傻瓜式的智能，或智能式的傻瓜。遗传算法与直接搜索工具箱有ga,gaoptimset,gaoptimget3个核心函数。ga函数（求解目标函数的最小值）1[x,fval,exitflag,output,population,scores]=ga(fitnessfcnmnvars,...options)%调用格式其中：fitnessfun适应度句柄函数nvars目标函数自变量个数options算法属性设置，通过函数gaoptimset赋予的x经过遗传进化以后自变量最佳染色体返回值fval最佳染色体的适应度exitflag算法停止的原因output输出的算法结构population最终得到种群适应度的列向量scores最终得到的种群gaoptimset函数gaoptimset函数是设置遗传算法的参数和句柄函数，常用的属性如下序号属性名默认值实现功能1PopInitRange[0;1]初始种群生成空间2PopulationSize20种群规模3CrossoverFraction0.8交配概率4Migration0.2变异概率5Generations100超过进化代数时算法停止6TimeLimitInf超过运算时间限制时算法停止7FitnessLimit-Inf最佳个体等于或小于适应度阈值时算法停止8StallGenLimit50超过连续代数不进化则算法停止9StallTimeLimit20超过连续时间不进化则算法停止10InitPopulation[]初始化种群11PlotFcns[]绘图函数1options=gaoptimset('param1',value1,'param2',value2,...)%调用格式gaoptimset该函数用于得到遗传算法参数结构中的参数具体指。1val=gaoptimset(options,'name')%调用格式option结构体变量name需要得到的参数名称val返回值###</li>
  <li>​大致流程图如下：st=&gt;start:确定实际问题参数code=&gt;operation:对参数集进行编码init=&gt;operation:初始化群体P(t)judge=&gt;operation:评价群体con=&gt;condition:满足停止规则mutation=&gt;operation:遗传操作以及产生新一代群体end=&gt;end:结束st-&gt;code-&gt;init-&gt;judge-&gt;concon(yes)-&gt;endcon(no)(bottom)-&gt;mutationmutation-&gt;init-&gt;judge以求函数$f(x)=9\sin{5x}+8\cos{4x}$的最大值为例，讲解遗传算法的过程初始化（编码）popsize表示群体大小，chromlength表示染色体长度（二进制数的长度），长度大小取决于变量的二进制编码的长度。123functionpop=initpop(popsize,chromlength)pop=round(rand(popsize,chromlength));end目标函数值1.二进制转化为十进制数123456789functionpop2=decodebinary(pop)[px,py]=size(pop);%求pop的行数和列数fori=1:pypop1(:,i)=2.^(py-i)*pop(:,i);endpop2=sum(pop1,2);%求pop1的每行之和end2.二进制编码转化为十进制数，spoint表示待解码的二进制串的起始位置。12345%将二进制编码转换成十进制functionpop2=decodechrom(pop,spoint,length)pop1=pop(:,spoint:spoint+length-1);pop2=popdecodebinary(pop1);end3.计算目标函数值12345function[objvalue]=calobjvalue(pop)temp1=decodechrom(pop,1,10);x=temp1*10/1023;objvalue=10*sin(5*x)+7*cos(4*x);end计算个体适应值1234567891011121314%计算个体适应值funcitonfitvalue=calfitvalue(objvalue)globalCmin;Cmin=0;[px,py]=size(objvalue);fori=1:pxifobjvalue(i)+Cmin&gt;0temp=Cmin+objvalue(i);elsetemp=0.0;endfitvalue(i)=temp;endfitvalue=fitvalue'选择复制选择或者说复制操作是决定那些个体可以进入下一代，程序中采用赌轮盘选择法选择，此方法较易实现。根据方程$p_i=\frac{f_i}{\sum{f_i}}$,选择步骤如下1.在第t代，计算$\sum{f_i}$和$p_i$2.产生{0，1}的随机数rand()，求s=rand(·)*$\sum{f_i}$3.求$\sum_{i=1}^{k}{fi}\geqs$中最小的k,则第k个个体被选中4.进行N次(2)、(3)操作，得到N个个体，成为第$t=t+1$代种群1234567891011121314151617%选择复制function[newpop]=selection(pop,fitvalue)totalfit=sum(fitvalue);fitvalue=fitvalue/totalfit;fitvalue=cumsum(fitvalue);[px,py]=size(pop);ms=sort(rand(px,1));fitin=1;newin=1;;whilenewin&lt;=pxif(ms(newin))&lt;fitvalue(fitin)newpop(newin)=pop(fitin);newin=newin+1;elsefitin=fitin+1;endend交叉1234567891011121314%交叉function[newpop]=crossover(pop,pc)[px,py]=size(pop);newpop=ones(size(pop));fori=1:2:px-1if(rand&lt;pc)cpoint=round(rand*py);newpop(i,:)=[pop(i,1:cpoint),pop(i+1,cpoint+1:py)];newpop(i+1,:)=[pop(i+1,1:cpoint),pop(i,cpoint+1:py)];elsenewpop(i,:)=pop(i);newpop(i+1,:)=pop(i+1);endend变异12345678910111213141516171819function[newpop]=mutation(pop,pm)[px,py]=size(pop);newpop=ones(size(pop));fori=1:pxif(rand&lt;px)mpoint=round(rand*py);ifmpoint&lt;=0mpoint=1;endnewpop(i)=pop(i);ifany(newpop(i,mpoint))==0newpop(i,mpoint)=1;elsenewpop(i,mpoint)=0;endelsenewpop(i)=pop(i);endend求出群体中最大适应值及其个体12345678910function[bestindividual,bestfit]=best(pop,fitvalue)[px,py]=size(pop);bestindividual=pop(1,:);bestfit=fitvalue(1);fori=2:pxiffitvalue(i)&gt;bestfitbestindividual=pop(i,:);bestfit=fitvalue(i);endendmain()123456789101112131415161718192021222324clearallclcpopsize=20;%群体大小chromlength=10;%字符串长度（个体长度）pc=0.7;%交叉概率pm=0.005%变异概率pop=initpop(popsize,chromlength);%随机产生初始群体fori=1:20[objvalue]=calobjvalue(pop);%计算目标函数fitvalue=calfitvalue(objvalue);%计算群体中每个个体的适应度[newpop]=selection(pop,fitvalue);%复制[newpop]=crossover(pop,pc);%交叉[newpop]=mutation(pop,pc);%变异[bestindividual,bestfit]=best(pop,fitvalue);%求出群体中适应值最大的个体及其适应值y(i)=max(bestfit);n(i)=i;pop5=bestindividual;x(i)=decodechrom(pop5,1,chromlength)*10/1023;pop=newpop;endfplot('9*sin(5*x)+8*cos(4*x)',[015]);holdonplot(x,y,'r*')holdofftips遗传算法有四个参数需要提前设定，一般在一下范围内进行设置。群体大小：20~100（太小会出现近亲交配，产生病态基因）遗传算法的终止进化代数：100~500（太小不容易收敛，太大有可能过于早熟不可能再收敛，继续进化无意义）交叉概率：0.4~0.99变异概率：0.0001~0.1（太大容易错失最优个体，太小不能有效更新种群）下一part讲matlab自带的遗传算法工具箱，这个明白大致原理即可。###</li>
  <li>本来是打算一篇写完遗传算法的，后来学着发现并不是那么好总结，后来又发现了Matlab的工具箱。。基本概念遗传算法是模拟达尔文生物进化论的自然选择和遗传学机理的生物进化过程的计算模型，是一种通过模拟自然进化过程搜索最优解的方法。遗传算法中的术语染色体又可称为基因型个体（individuals），一定数量的的个体组成了群体（population），群体中个体数量的称为群体大小。基因基因（gene）是串中的元素，基因用于表示个体的特征。例如有一个串S=1011，则其中的1，0，1，1这四个元素分别称为基因。他们的值称为等位基因（alleles）。基因位点基因位点（locus）在算法中表示一个基因在串中的位置，称为基因位置（geneposition），有时也称基因位。基因位置由串的左向右计算，如在串S=1101中，0的基因位置是3.特征值在用串表示整数时，基因的特征值（feature）与二进制数的权一致。例如在串S=1011中，基因位置3中的1，他的基因特征值为2；基因位置中的1，他的基因特征值为8.适应度各个个体对环境的适应程度称为适应度（fitness）。为了体现染色体的适应能力，引入了对问题中的每一个染色体哦都能进行度量的函数，叫适应度函数。这个函数是计算个体在群体中被适用的概率。程序设计12345678910111213Begint=0初始化P(t);计算P(t)的适应值;while(不满足停止规则)dobegint=t+1;从P(t+1)中选择P(t)重组P(t)计算P(t)的适应值end大致流程如上，下一part解释具体算法。还有好多东西没学啊，灰色预测，线性规划，神经网络，非线性规划，图论，机器学习，粒子群算法，免疫算法，模糊控制，时间序列分析，啊啊啊啊啊啊啊啊啊，学不完了啊。</li>
  <li>WhatisT检验？T检验，亦称studentt检验（Student’sttest），主要用于样本含量较小（例如n&lt;30），总体标准差σ未知的正态分布。t检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。T检验可用于比较均值，检查抽样样本是否来自一已知总体。要详细解释t检验，首先引入一些统计学的基本原理知识。假设检验对提出的一些总体假设进行分析判断，做出统计决策。Why检验？通过获得随机样本来实施抽样研究的例子很多，但此时研究中直接获取的只是样本的情况，而研究者关心的并不仅仅是样本，更希望了解相应的总体特征。原理基础：小概率原理，即一般认为小概率事件在一次随机抽样中不会发生基本思想：先建立一个关于样本所属总体的假设，考察在假设条件下随机样本的特征信息是否属小概率事件，若为小概率事件，则怀疑假设成立有悖于该样本所提供特征信息，因此拒绝假设事实上，小概率事件在随机抽样中还是可能发生的，只是发生的概率很小。若正好碰上了，则假设检验的结论就是错误的。当然，犯这种错误的概率很小基本步骤建立假设：根据统计推断的目的而提出的对总体特征的假设。统计学中的假设有两方面的内容：一是检验假设(hypothesistobetested),亦称原假设或无效假设(nullhypothesis)，记为H0；二是与H0相对立的备择假设(alternativehypothesis)，记为H1。后者的意义在于当H0被拒绝时供采用。两者是互斥的，非此即彼。H0：μ=μ0，H1：μ≠0；H0：μ=7.4，H1：μ≠7.4。确定检验水准实际上就是确定拒绝H0时的最大允许误差的概率。检验水准(sizeoftest)，常用α表示，是指检验假设H0本来是成立的，而根据样本信息拒绝H0的可能性大小的度量，换言之，α是拒绝了实际上成立的H0的概率。常用的检验水准为α=0.05，其意义是：在所设H0的总体中随机抽得一个样本，其均数比手头样本均数更偏离总体均数的概率不超过5%。计算检验统计量和P值实际上在此之前还有一步叫做进行试验，所需的样本数据即从此得来统计量只是工具，概率值才是目的，它可以客观衡量样本对假设总体偏离程度从H0假设的总体中抽出现有样本（及更极端情况）的概率，即P值例如600次赢100次是H0假设的情况，只赢1次就是现有样本情况，更极端的情况就是连一次也没有赢检验统计量的特点该统计量应当服从某种已知分布，从而可以计算出P值各种检验方法所利用的分布及计算原理不同，从而检验统计量也不同得出推断结论按照事先确定的检验水准α界定上面得到的P值，并按小概率原理认定对H0的取舍，作出推断结论if(p&lt;α)基于H0假设的总体情况出现了小概率事件则拒绝H0，接受H1，可以认为样本与总体的差别不仅仅是抽样误差造成的，可能存在本质上的差别，属“非偶然的(significant)”，因此，可以认为两者的差别有统计学意义。else基于H0出现了很常见的事件则样本与总体间的差别尚不能排除纯粹由抽样误差造成，可能的确属“偶然的(non-significant)”，故尚不能拒绝H0因此，认为两者的差别无统计学意义，但这并不意味着可以接受H0。应该注意的一些问题结论不能绝对化本身就保留了犯错误的可能性样本量导致的检验效能问题样本量太小，导致检验效能不足，从而无法检出可能存在的差异样本量太大，得出的有统计学意义的结论可能根本就没有实际意义单样本t检验\[t=\frac{\bar{x}-\mu}{s_{\bar{x}}}\]单样本t检验是检验一个样本平均数与一个已知的总体平均数的差异是否显著。当总体分布是正态分布，如总体标准差未知且样本容量小于30，那么样本平均数与总体平均数的离差统计量呈t分布。​x：样本平均数​μ：已知总体平均数​s：标准误适用条件因为有中心极限定理，一般均数的抽样分布都不会有问题，真正会限制该方法使用的是均数是否能够代表相应数据的集中趋势。也就是说，只要数据分布不是强烈的偏态，一般而言单样本t检验都是适用的。Bootstrap抽样可以在一定程度下解决这个问题完全随机的两样本t检验目的：推断两个样本是否来自相同的总体，更具体地说，是要检验两样本所代表的总体均数是否相等。检验假设：无效假设H0：μ1=μ2备择假设H1：μ1≠μ2检验水准α=0.05原理\[t=\frac{\bar{x}_1-{\bar{x}_2}}{s_{\bar-{\bar{x_2}}}}\]适用条件正态性：有一定的耐受能力，可以通过直方图等进行观察，偏的不厉害就行注意应当分组考察独立性：对结果的影响较大，但一般没问题方差齐性：相对而言对结论的影响较大，需要进行方差齐性检验配对t检验原理\[t=\frac{\bar{D}}{S/\sqrt{n}}\]配对设计的两种情况对同一个受试对象处理前后的比较将受试对象按情况相近者配对（或者自身进行配对），分别给予两种处理，以观察两种处理效果有无差别。配对设计的特点在配对设计得到的样本数据中，每对数据之间都有一定的相关，如果采用成组的t检验就无法利用这种关系，浪费了大量统计信息对于这种情况，统计学上的解决办法是求出每对的差值，通过检验该差值总体均数是否为0，就可以得知两种处理有无差异。参考资料张文彤《SPSS20.0统计分析基础教程第2版》</li>
  <li>先写在这里提醒我自己，以后写写东西随手按一下ctrl+s。。。定义聚类分析指将物理或抽象对象的集合分组为由类似的对象组成的多个类的分析过程。在数学，计算机科学，经济学，统计学，以及机器学习领域有着广泛的应用。与分类的区别数据分类是分析已有的数据，寻找其共同的属性，并根据分类模型将这些数据划分成不同的类别，这些数据赋予类标号。这些类别是事先定义好的，并且类别数是已知的。相反，数据聚类则是将本没有类别参考的数据进行分析并划分为不同的组，即从这些数据导出类标号。聚类分析本身则是根据数据来发掘数据对象及其关系信息，并将这些数据分组。每个组内的对象之间是相似的，而各个组间的对象是不相关的。不难理解，组内相似性越高，组间相异性越高，则聚类越好。聚类是无监督学习，就是把未标记的数据集通过算法的方式加以标记，而分类是监督学习。常用算法k-means算法（快速聚类）定义 k-means算法是一种简单的迭代型聚类算法，采用距离作为相似性指标，从而发现给定数据集中的K个类，且每个类的中心是根据类中所有值的均值得到，每个类用聚类中心来描述。对于给定的一个包含n个d维数据点的数据集X以及要分得的类别K,选取欧式距离作为相似度指标，聚类目标是使得各类的聚类平方和最小，即最小化：\[J=\sum_{k=1}^k\sum_{i=1}^n\|x_i-u_k\|^2\]​结合最小二乘法和拉格朗日原理，聚类中心为对应类别中各数据点的平均值，同时为了使得算法收敛，在迭代过程中，应使最终的聚类中心尽可能的不变。算法流程K-means是一个反复迭代的过程，算法分为四个步骤：1）选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心；2）对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则（贪心）将它们分到距离它们最近的聚类中心（最相似）所对应的类；3）更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值；4）判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）(迭代)spss操作及注意事项分析–&gt;分类–&gt;k均值聚类–&gt;选定聚类数(K)–&gt;(迭代–&gt;设定最大迭代次数，大一点无妨)由于数据的量纲不同，所以有些情况下会对聚类结果造成较大误差，所以需要做一下标准化：分析–&gt;描述统计–&gt;描述(勾选将标准化得分另存为变量)小结算法原理简单，只需要一个参数k，但是这个参数k在有些情况下是不容易选定的受初值影响较大在大数据样本下也能保证一定的准确性，但是算法的时间复杂度较高对离群点较敏感K-modes算法可以实现对离散数据的快速聚类层次聚类（系统聚类）HierarchicalClusteringK-means算法是一种方便好用的算法，但是有K值选择和聚类中心点选择的问题，会在一定程度上影响聚类效果，为了避免这种问题可以采用层次聚类。首先考虑欧式空间下的层次聚类。该算法仅可用于规模相对较小的数据集。层次聚类用于非欧式空间时，还有一些与层次聚类相关的额外问题需要考虑。因此，当不存在簇质心或者说簇平均点时，可以考虑采用簇中心点（clustroid）来表示一个簇。spssspss下仅提供了合并法，大致过程如下首先将各聚类单位各自作为一类(此时有n类)，按照所定义的距离计算各数据点之间的距离，形成一个距离阵将距离最近的两个单位并为一个类别，形成n-1个类别，计算新产生的类别与其他各类别之间的距离或者相似度（这涉及如何计算两个类别之间距离或者相似度的问题，习惯上使用Pearson相关系数），形成新的距离阵。按照和第二部相同的原则，再将距离最接近的两个类别合并，这是如果类别个数仍然大于1，则重复该步骤，直到所有数据都被合并成一个类别为止。层级聚类法的优点非常明显：可以对变量或者case进行聚类，变量可以为连续或分类变量，提供的距离测量方法和结果表示方法也非常丰富，但是，由于要反复计算距离，当样本量过大或者变量较多时，采用层次聚类运算速度明显较慢，不过现金计算机硬件水平提高已经使得其不再明显。进一步的讨论各种层次聚类方法在层次聚类法中，当每个类别由多于一个的数据点构成时，就会涉及如何定义两个类别间距离的问题，距离的定义的不同会得到不同的分析结果，这也就构成了不同的层次聚类方法，常用的有一下几种。(1).最短距离法、中位数法、最长距离法(2).重心法(3).组内连接法、组间连接法(4).Ward法实践证明，默认的组间平均距离法就是一种非常优秀的方法，使用即可利用标准化来调整聚类模式以案例(case)聚类为例，按照默认设定，分析的基本模式是保证用于聚类的变量在类别内的取值差异大。事实上，这只是聚类分析的一种角度，另一个角度，就是按照变量取值的变化模式来进行聚类。在spss中实现按照变量取值的变化模式来进行聚类是通过对变量进行不同标准化的方式来进行的，在spss中，默认的变量标准化是以变量的均数和标准差等统计量作为参照来进行的。如果选择了以案例的均属和标准差等统计量作为参照来进行并进行聚类，就会得到按照数据的取值模式而得到的聚类结果。具体在操作上，出对数据进行行列转置外，还可以通过将变量标准化的方法改为按照案例来进行，即在“方法”对话框左下角的“转换值”选项组中，要求数据“按个案”而不是“按变量”进行聚类即可。参考文献：[1]张文彤《spss20.0统计分析高级教程第二版》288页[2]https://www.cnblogs.com/xmeo/p/6543057.html</li>
</ul>