---
layout:     post
title:      "cs224n lecture2"
subtitle:   " \"word2vec basic concept\""
date:       2019-09-05 20:00:00
author:     "Rumia"
catalog: true
category: blog
tags:
    - nlp
    - cs224n
---



>  终于是花了三四天时间断断续续看完了cs224n lecture2，难度真的是指数型上升。



## 基本原理

### 词嵌入

- 什么是词嵌入？

  自然语言不像图像信息一样，自然语言的基本单位是文字，在英文中是word，中文中是字，他们都是有具体意义的抽象表示，而所谓图像处理，处理的本身就是图像信号，图像本身在采集或者创建好就是以数字形式存储在计算中。但是，重点来了，要使用统计的方法处理自然语言，就必须以一定的形式量化编码词，说简单一点，就是把符号形式的文字转化成数值，或者说嵌入到一个数学空间里，建立一个从文字到数学空间的映射。也就是所谓的词嵌入，而这里要将到的 [**word2vec**](https://en.wikipedia.org/wiki/Word2vec)便是常用的词嵌入的一种。**也就是说，词嵌入就是把词进行了数值化的表示。**

- 如何进行词嵌入？

  **word2vec** 是一种典型的词嵌入方法，使用一个全连接网络（输入层，隐含层，输出层）得到一个训练好的可以表征词向量的模型。

### wrod2vec的一些具体含义

- 比较直观的理解

  比较通俗的解释方法是，我们把一个词转化成所谓向量，也就是把这个词进行了量化表示，一个比较直观的例子，颜色。我们都知道，颜色在计算机中通常使用rgb，也就是红绿蓝三原色进行编码，很自然的可以通过一个三维向量，比如蓝色（0，0，255），这就是一个经过编码的空间向量，但如果我们不知道所谓的蓝色的向量化表示呢？这就是word2vec的意义所在，将蓝色这个直观意义上（假设）的**词语**，映射到一个三维的向量空间，也就是我们通常所称的rgb色彩空间。
  
  也就是：蓝色  -> （0，0，255）
  
  ​			   红色   ->   (255，0，0）
  
  ​			   绿色   -> （0，255，0）
  
  假设上边三个向量是我们训练出来的，而不是一种基于确定标准的量化，那么有意思的事情来了。
  
  我们知道，蓝色 + 绿色 = 青色，那么，如果我们拿 青色 这个词语放到神经网络里面去训练，得出的对应青色的向量，应该是大致就是（0，255，255），这就意味着，我们通过神经网络，找到了所谓的颜色之间的量化关系（大致上）也就是完成了所谓的词嵌入。
  
  当然，事实上，颜色并不是这么来的，图像信息作为一种天然可以被计算机量化的信息（曝光），原则上来说根本不需要这么个对颜色词语的词嵌入，这里只是为了举例。
  
  类似的，当我们拥有下面的词向量时：
  
  "Beijing"、"Tokyo"、"China"、"Japan"。
  
  如果训练的足够合理的话，我们应该能大致得到如下的关系
  $$
  \vec{Beijing}-\vec{China}+\vec{Japan} = \vec{Tokyo}
  $$
  所以，词向量中蕴含是词语之间真实关系的表述，也就是说，当北京有首都的属性和中国的属性，当二者相减时，我们就可以神奇的得到了一个可以表征首都属性的向量，很明显，当加上日本时，就出现了Tokyo。
  
  这就是word2vec的强大之处，将客观的词语进行向量化，从而使得原本单一的词语字符串可以进行数学运算，也就是实现了词嵌入。
  
- 一些需要注意的问题

  我们通过神经网络训练出来的词向量，理论上来说，我们并不容易知道每个维度其对应的确切意义所在。Manning在第一节课上也讲了这一点，这也就是所谓的黑箱（Black Box），黑箱这个东西的问题就在于，如果我们对黑箱的输入得到了正确的输出，那很好；但如果我们得到了错误的输出，那对于一般系统来说，可以通过调整参数来对系统进行修正，但调整参数来修正黑箱的性能似乎并不容易。典型的黑箱系统就是神经网络。如果我们仅仅只是要求了大量数据样本下的整体准确率，那很好，一个优秀的神经网络能够很高的准确率与性能，而且设计起来的成本也较一般传统方法大大降低。但是如果除了错就有可能造成不可挽回的后果，那么就必须谨慎使用这个东西了。所以，神经网络的可解释性也是一个非常重要的话题。这个东西，毕竟是一个基于统计学习训练出来的模型。（个人看法）

## 网络原理

### 神经网络

- 基本结构

  这部分确实看着很复杂，下面用例子简单说明

  我们假设要训练一个十个词语的 word2vec 

  {'apple','drink','eat','juice','milk','orange','rice','water'}

  我们基本的网络结构如下：

  ![](../img/neural_structure.png)

  如图所示，该网络是一个由10个输入节点的输入层，5个隐含节点的隐含层，10个输出节点的输出层的全连接网络。

- 参数推导

  我们以连续单上下文词预测单目标词的 CBOW（Continuous Bag-of-Word Model 连续词袋模型）进行参数的推导演示。

  在输入层和输出层